# ğŸ’¬ Talk to Your Data

A production-grade conversational analytics platform that lets you upload spreadsheets and ask questions in natural language. Powered by LangGraph, GPT-4, and pandas.

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115-green.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.41-red.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue.svg)
![Redis](https://img.shields.io/badge/Redis-7-red.svg)

---

## âœ¨ Features

### Core Functionality
- **ğŸ“ Multi-file Upload** - Upload CSV and Excel files with automatic schema detection
- **ğŸ’¬ Natural Language Queries** - Ask questions like "What's the average revenue by region?"
- **ğŸ”„ Cross-table Analysis** - Compare data across multiple files and time periods
- **ğŸ“Š Interactive Charts** - Visualize results with Plotly charts
- **ğŸ§  AI-Powered Insights** - Get key findings, recommendations, and actionable insights

### Technical Features
- **ğŸ”— LangGraph Pipeline** - Structured reasoning flow with retry logic
- **âš¡ Redis Caching** - Cache analysis results for instant repeated queries
- **ğŸš¦ Rate Limiting** - Protect against abuse (60 req/min, 1000 req/hr)
- **ğŸ“¦ Background Tasks** - Celery workers for heavy processing
- **ğŸ”Œ Connection Pooling** - Optimized database connections
- **ğŸ“ Structured Logging** - JSON logs for observability

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit     â”‚â”€â”€â”€â”€â–¶â”‚    FastAPI      â”‚â”€â”€â”€â”€â–¶â”‚   PostgreSQL    â”‚
â”‚   Frontend      â”‚     â”‚    Backend      â”‚     â”‚   Database      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   LangGraph     â”‚
                        â”‚   Pipeline      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                                     â–¼                  
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    
     â”‚   OpenAI    â”‚                       â”‚    Redis    â”‚    
     â”‚   GPT-4     â”‚                       â”‚    Cache    â”‚   
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    
```

### LangGraph Pipeline Flow

```
User Query â†’ Ingest â†’ Plan â†’ Generate Code â†’ Validate â†’ Execute â†’ Explain â†’ Response
                â†“                                          â†‘
            [Retry on failure with error context] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Prerequisites

- **Python 3.11+**
- **Docker & Docker Compose** (for PostgreSQL and Redis)
- **OpenAI API Key** (or Anthropic/Ollama)

---

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd Data_Analysis_Agent

# Create environment file
cp env.example .env
```

### 2. Configure Environment

Edit `.env` with your settings:

```bash
# Required: LLM API Key
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_MODEL=gpt-4o-mini

```

### 3. Start Infrastructure & Applications

```bash
# Start Backend, Frontend , PostgreSQL , Redis and db Migration
docker-compose up --build
```


### 4. Access the Application

- ##### Open the frontend in browser to access the application

- **Frontend**: http://localhost:8501
- **API Docs**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

#### Everything is running inside Docker â€” no need to manually set up virtual environments.
---


## ğŸ“– Usage Guide

### Step 1: Upload Data Files

1. Navigate to **ğŸ“ Upload** page
2. Drag and drop your CSV or Excel files
3. The system automatically detects:
   - Column types (numeric, categorical, date)
   - Time periods from filenames (e.g., `sales_nov_2024.csv` â†’ "Nov 2024")

### Step 2: Ask Questions

Navigate to **ğŸ’¬ Chat** and ask questions like:

| Query Type | Example |
|------------|---------|
| **Aggregation** | "What is the average revenue by region?" |
| **Comparison** | "Compare sales between November and December 2024" |
| **Ranking** | "Show top 5 products by units sold" |
| **Filtering** | "Total revenue for APAC region only" |
| **Trends** | "How did revenue change over time?" |

### Step 3: View Results

- **ğŸ“Š Charts** - Interactive Plotly visualizations
- **ğŸ“‹ Tables** - Detailed data tables
- **ğŸ’¡ Insights** - AI-generated key findings and recommendations

### Step 4: Review History

Navigate to **ğŸ“Š Analysis** to:
- View past analyses
- Download generated Python code
- Export results as CSV

---


## ğŸ“ Project Structure

```
pushkal/
â”œâ”€â”€ backend/                    # FastAPI Backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/               # API Routes
â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py      # File upload endpoints
â”‚   â”‚   â”‚   â””â”€â”€ chat.py        # Chat endpoints
â”‚   â”‚   â”œâ”€â”€ core/              # Core utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py      # Settings management
â”‚   â”‚   â”‚   â”œâ”€â”€ cache.py       # Redis caching
â”‚   â”‚   â”‚   â”œâ”€â”€ middleware.py  # Rate limiting
â”‚   â”‚   â”‚   â””â”€â”€ celery_app.py  # Task queue
â”‚   â”‚   â”œâ”€â”€ models/            # SQLAlchemy Models
â”‚   â”‚   â”‚   â”œâ”€â”€ session.py     # Session model
â”‚   â”‚   â”‚   â”œâ”€â”€ file.py        # UploadedFile model
â”‚   â”‚   â”‚   â”œâ”€â”€ message.py     # ChatMessage model
â”‚   â”‚   â”‚   â””â”€â”€ analysis.py    # AnalysisResult model
â”‚   â”‚   â”œâ”€â”€ schemas/           # Pydantic Schemas
â”‚   â”‚   â”œâ”€â”€ services/          # Business Logic
â”‚   â”‚   â”‚   â”œâ”€â”€ file_service.py
â”‚   â”‚   â”‚   â””â”€â”€ chat_service.py
â”‚   â”‚   â”œâ”€â”€ tasks/             # Celery Tasks
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py    # Background analysis
â”‚   â”‚   â”‚   â””â”€â”€ cleanup.py     # Maintenance tasks
â”‚   â”‚   â””â”€â”€ main.py            # Application entry
â”‚   â”œâ”€â”€ alembic/               # Database migrations
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ alembic.ini
â”‚
â”œâ”€â”€ frontend/                   # Streamlit Frontend
â”‚   â”œâ”€â”€ app.py                 # Main application
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ 1_ğŸ“_Upload.py     # Upload page
â”‚   â”‚   â”œâ”€â”€ 2_ğŸ’¬_Chat.py       # Chat page
â”‚   â”‚   â””â”€â”€ 3_ğŸ“Š_Analysis.py   # History page
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ pipeline/                   # LangGraph Pipeline
â”‚   â”œâ”€â”€ graph.py               # State graph definition
â”‚   â”œâ”€â”€ state.py               # GraphState TypedDict
â”‚   â”œâ”€â”€ llm.py                 # LLM configuration
â”‚   â””â”€â”€ nodes/                 # Pipeline nodes
â”‚       â”œâ”€â”€ ingest.py          # Query ingestion
â”‚       â”œâ”€â”€ planning.py        # Intent analysis
â”‚       â”œâ”€â”€ code.py            # Code generation
â”‚       â””â”€â”€ explain.py         # Result explanation
â”‚
â”œâ”€â”€ data/                       # Sample data files
â”‚   â”œâ”€â”€ sales_nov_2024.csv
â”‚   â”œâ”€â”€ sales_dec_2024.csv
â”‚   â””â”€â”€ sales_q1_2025.csv
â”‚
â”œâ”€â”€ docker-compose.yml          # Infrastructure
â”œâ”€â”€ .env                        # Environment config
â””â”€â”€ README.md
```

---

## ğŸ”Œ API Reference

### Health Check
```bash
GET /health
```
Returns status of all services (API, Database, Redis).

### Upload File
```bash
POST /api/v1/upload
Content-Type: multipart/form-data

file: <file>
session_id: <string>
```

### List Files
```bash
GET /api/v1/files?session_id=<session_id>
```

### Send Chat Message
```bash
POST /api/v1/chat
Content-Type: application/json

{
  "session_id": "your-session-id",
  "message": "What is the average revenue?"
}
```

### Get Chat History
```bash
GET /api/v1/chat/history?session_id=<session_id>&limit=20
```

### Get Analysis Details
```bash
GET /api/v1/chat/analysis/{analysis_id}
```

---

## ğŸ§ª Testing

### Test File Upload
```bash
curl -X POST http://localhost:8000/api/v1/upload \
  -F "file=@data/sales_nov_2024.csv" \
  -F "session_id=test-session"
```

### Test Chat
```bash
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{"session_id": "test-session", "message": "What is the total revenue?"}'
```

### Run Unit Tests
```bash
cd backend
pytest tests/ -v
```

---

## ğŸ› Troubleshooting

### Common Issues

**"Cannot connect to PostgreSQL"**
```bash
# Check if Docker is running
docker-compose ps

# Restart PostgreSQL
docker-compose restart postgres
```

**"OPENAI_API_KEY not found"**
```bash
# Ensure .env file exists and has the key
cat .env | grep OPENAI
```

**"Redis connection failed"**
```bash
# Check Redis status
docker-compose logs redis

# The app works without Redis (caching disabled)
```

**"Module not found: pipeline"**
```bash
# Ensure you're in the project root
cd /path/to/pushkal
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

---

## ğŸ“ Sample Data

The `data/` folder contains sample CSV files for testing:

| File | Description | Rows | Period |
|------|-------------|------|--------|
| `sales_nov_2024.csv` | November sales | 10 | Nov 2024 |
| `sales_dec_2024.csv` | December sales | 12 | Dec 2024 |
| `sales_q1_2025.csv` | Q1 summary | 9 | Q1 2025 |

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [LangGraph](https://github.com/langchain-ai/langgraph) - Pipeline orchestration
- [FastAPI](https://fastapi.tiangolo.com/) - Backend framework
- [Streamlit](https://streamlit.io/) - Frontend framework
- [OpenAI](https://openai.com/) - LLM provider

---

**Built by Pushkal for conversational data analytics**
